{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CSC498 - Introduction to Natural Language Processing.\n",
    "Dr. Pauline Maouad\n",
    "SAS, Lebanese American University.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import * \n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer #regular expression tokenizer\n",
    "import re  # regular expression module\n",
    "\n",
    "\n",
    "from nltk.corpus import gutenberg as g\n",
    "from nltk.corpus import brown as b\n",
    "from nltk.corpus import nps_chat\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/paulina/Documents/1.LAU/CSC498_NLP/Python_Code'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match!\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"Cookie\"\n",
    "string = \"Cookie\" # would be the corpus you are searching in to find a given pattern\n",
    "\n",
    "if re.match(pattern, string):\n",
    "    print(\"Match!\")\n",
    "else: print(\"Not a match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the *r* character refers to *raw string literal*, which means it changes how the string is interpreted. Such literals are stored as they appear.\n",
    "For example, \\ is just a backslash when prefixed with an r rather than being interpreted as an escape sequence. You will see what this means with special characters. Sometimes, the syntax involves backslash-escaped characters, and to prevent these characters from being interpreted as escape sequences; you use the raw r prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 1:  howdy\n",
      "how are ' you\n"
     ]
    }
   ],
   "source": [
    "# example to show the usage of the r - raw string literal\n",
    "\n",
    "line1 = 'howdy\\nhow are \\' you'\n",
    "print(\"line 1: \", line1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "line2:  howdy\\nhow are \\' you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"howdy\\\\nhow are \\\\' you\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "\n",
    "line2 = r'howdy\\nhow are \\' you'\n",
    "print(\"line2: \", line2)\n",
    "\n",
    "line2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"howdy\\\\nhow are \\\\' you\""
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"howdy\\\\nhow are \\\\' you\"\n",
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Wildcard '.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Matches any single character except the newline character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='Couk*e'>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(r'Co.k.e', 'Couk*e')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='Cookie'>\n"
     ]
    }
   ],
   "source": [
    "#refer to powerpoint lecture. In case you want to match the regex anywhere in the string use search() intead of match()\n",
    "obj=re.match(r'Co.k.e', 'Cookie') \n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(11, 17), match='Cookie'>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj=re.search(r'Co.k.e', '...%^$%##% Cookie&^*^*&^ the lake ') \n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the actual string matched by the regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cookie'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'Co.k.e', 'Cookie').group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caret ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eat'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'^Eat', \"Eat cake!\").group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cake '"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('^Cake | cake$', \"Cake ! Let's eat cake\").group() \n",
    "\n",
    "#What if we wanted to return all occurrences of words 'Cake' or 'cake'??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-6fc2e258001a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'cake$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Let's get some cake on our way home!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "re.match(r'cake$', \"Let's get some cake on our way home!\").group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'[0-6]', 'Number: 5').group() #search for any digit between 0-6 in the string \"Number: 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'Number: [^5]', 'Number: 9').group()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase - Uppercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lowercase:** \\w: any ONE WORD character. Matches any single letter, digit, or underscore. For ASCII, word characters are [a-zA-Z0-9_] \n",
    "\n",
    "**Uppercase:** \\W: any NON-WORD character. Matches any character not part of \\w (lowercase w)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lowercase w:\", re.search(r'Co\\wk\\we', 'CoOkie').group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches any character except single letter, digit or underscore\n",
    "print(\"Uppercase W:\", re.search(r'Co\\Wke', 'Co*ke').group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase W won't match, and return:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "## Uppercase W won't match single letter, digit\n",
    "print(\"Uppercase W won't match, and return:\\n\", re.search(r'Co\\Wk\\We', 'Cookie'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n",
      "Yes\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "# Find sequences of at least one upper case letter followed by lower case letters \n",
    "\n",
    "# Function to match the string \n",
    "def match(text): \n",
    "    # regex \n",
    "    \n",
    "    pattern = '[A-Z]+[a-z]+$'\n",
    "    # searching pattern \n",
    "    if re.search(pattern, text): \n",
    "        return('Yes') \n",
    "    else: \n",
    "        return('No') \n",
    "\n",
    "# Driver Function \n",
    "print(match(\"GeeeeeKKs\")) \n",
    "print(match(\"geeksSforGeeks\")) \n",
    "print(match(\"geeks\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lowercase:** \\s: 's'. Matches a single whitespace character like: space, newline, tab, return.\n",
    "\n",
    "**Uppercase:** \\S: 'S'. Matches any character not part of \\s (lowercase s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase s: Eat cake\n",
      "Uppercase S: cookie\n"
     ]
    }
   ],
   "source": [
    "print(\"Lowercase s:\", re.search(r'Eat\\scake', 'Eat cake').group())\n",
    "print(\"Uppercase S:\", re.search(r'cook\\Se', \"Let's eat cookie\").group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lowercase d:** \\d. Matches decimal digit 0-9.\n",
    "\n",
    "**Uppercase d:** \\d.  Matches any character that is not a decimal digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many cookies do you want?  87\n"
     ]
    }
   ],
   "source": [
    "print(\"How many cookies do you want? \", re.search(r'\\d+', '87 cookies').group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other examples**\n",
    "\n",
    "\\t - Lowercase t. Matches tab.\n",
    "\n",
    "\\n - Lowercase n. Matches newline.\n",
    "\n",
    "\\r - Lowercase r. Matches return.\n",
    "\n",
    "\\A - Uppercase a. Matches only at the start of the string. Works across multiple lines as well.\n",
    "\n",
    "\\Z - Uppercase z. Matches only at the end of the string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repetitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cooookie'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'Co+kie', 'Cooookie').group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cookie'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'Ca*o*kie', 'Cookie').group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Color'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'Colou?r', 'Color').group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {x} - Repeat exactly x number of times.\n",
    "### {x,} - Repeat at least x times or more.\n",
    "### {x, y} - Repeat at least x times but no more than y times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'09876'"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'\\d{3,5}', '0987654321').group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping in Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire Email address: helpdesk@lau.edu.lb\n",
      "Username: helpdesk\n",
      "Host: lau.edu.lb\n"
     ]
    }
   ],
   "source": [
    "# extract username or domain name from an email\n",
    "\n",
    "statement = 'Please contact us at: helpdesk@lau.edu.lb'\n",
    "\n",
    "match = re.search(r'([\\w\\.-]+)@([\\w\\.-]+)', statement)\n",
    "\n",
    "if statement:\n",
    "    print(\"Entire Email address:\", match.group(0)) # The whole matched text\n",
    "    print(\"Username:\", match.group(1)) # The username (group 1)\n",
    "    print(\"Host:\", match.group(2)) # The host (group 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email address: helpdesk@lau.edu.lb\n",
      "Username: helpdesk\n",
      "Host: lau.edu\n",
      "Country: .lb\n"
     ]
    }
   ],
   "source": [
    "statement = 'Please contact us at: helpdesk@lau.edu.lb'\n",
    "match = re.search(r'([\\w\\.-]+)@([\\w\\.-]+)(\\.\\w+)', statement)\n",
    "if statement:\n",
    "    print(\"Email address:\", match.group()) # The whole matched text\n",
    "    print(\"Username:\", match.group(1)) # The username (group 1)\n",
    "    print(\"Host:\", match.group(2)) # The host (group 2)\n",
    "    print(\"Country:\", match.group(3)) # The host (group 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple: ('helpdesk@lau.edu.lb', 'helpdesk', 'lau.edu', '.lb')\n"
     ]
    }
   ],
   "source": [
    "# multiple arguments give a tuple\n",
    "statement = 'Please contact us at: helpdesk@lau.edu.lb'\n",
    "match = re.search(r'([\\w\\.-]+)@([\\w\\.-]+)(\\.\\w+)', statement)\n",
    "print(\"Tuple:\", match.group(0,1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below code corresponds to the **Simple Example** in slide no 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'the']"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = \"The theological planet the.\"\n",
    "re.findall(r'\\b[tT]he\\b', statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' the.']"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what if we want to find 'the' with some context, i.e. the_ or the25\n",
    "\n",
    "statement = \"The theological planet the.\"\n",
    "re.findall(r'[^a-zA-Z][tT]he[^a-zA-Z]', statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The '"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = \"The theological planet.\"\n",
    "re.search(r'((^|[^a-zA-Z])[tT]he([^a-zA-Z]|$))', statement).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8The'"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = \"The theological planet. The\"\n",
    "re.search(r'((^|[^a-zA-Z])[tT]he([^a-zA-Z]|$))', statement).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{The }theological planet{.The}\n"
     ]
    }
   ],
   "source": [
    "statement2 = \"The theological planet.The\"\n",
    "\n",
    "nltk.re_show(r'((^|[^a-zA-Z])[tT]he([^a-zA-Z]|$))', statement2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## White space tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "\n",
    "print(WhitespaceTokenizer().tokenize(s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordpunct tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPunctTokenizer splits all punctuations into separate tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "print(WordPunctTokenizer().tokenize(s))\n",
    "\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "print(word_tokenize(s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's different from word_tokenize that keeps some punctuations with the word instead of separating them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'm', 'a', 'dog', 'and', 'it', \"'\", 's', 'great', '!', 'You', \"'\", 're', 'cool', 'and', 'Sandy', \"'\", 's', 'book', 'is', 'big', '.', 'Don', \"'\", 't', 'tell', 'her', ',', 'you', \"'\", 'll', 'regret', 'it', '!', \"'\", 'Hey', \"',\", 'she', \"'\", 'll', 'say', '!']\n",
      "\n",
      "\n",
      "['I', \"'m\", 'a', 'dog', 'and', 'it', \"'s\", 'great', '!', 'You', \"'re\", 'cool', 'and', 'Sandy', \"'s\", 'book', 'is', 'big', '.', 'Do', \"n't\", 'tell', 'her', ',', 'you', \"'ll\", 'regret', 'it', '!', \"'Hey\", \"'\", ',', 'she', \"'ll\", 'say', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "# from nltk.tokenize import PunktWordTokenizer\n",
    "from nltk import word_tokenize # a wrapper function that calls tokenize() on an instance of class TreeBankWordTokenizer\n",
    "\n",
    "# notice the contractions.\n",
    "s = \"I'm a dog and it's great! You're cool and Sandy's book is big. Don't tell her, you'll regret it! 'Hey', she'll say!\" \n",
    "\n",
    "\n",
    "print(WordPunctTokenizer().tokenize(s))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(word_tokenize(s))  # notice minor issue in word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "text = webtext.raw('overheard.txt')\n",
    "sentpunct_tokenizer = PunktSentenceTokenizer(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct_tokenizer:  White guy: So, do you have any plans for this evening?\n",
      "default tokenizer:White guy: So, do you have any plans for this evening? \n",
      "\n",
      "punct_tokenizer: Girl: But you already have a Big Mac...\n",
      "default tokenizer: Girl: But you already have a Big Mac...\n",
      "Hobo: Oh, this is all theatrical. \n",
      "\n",
      "punct_tokenizer: Hobo: Oh, this is all theatrical.\n",
      "default tokenizer: I only have a dollar...Can you spare some change? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sents1 = sentpunct_tokenizer.tokenize(text)\n",
    "print(\"punct_tokenizer: \", sents1[0])\n",
    "\n",
    "sents2 = sent_tokenize(text)\n",
    "print(\"default tokenizer:%s \\n\" % sents2[0])\n",
    "\n",
    "print(\"punct_tokenizer:\", sents1[678]) # better tokenization of sentences\n",
    "print(\"default tokenizer: %s \\n\" % sents2[678])\n",
    "\n",
    "\n",
    "print(\"punct_tokenizer:\", sents1[679])\n",
    "print(\"default tokenizer: %s \\n\" % sents2[679])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Can't\", 'beat_', 'that', '_', 'sound']\n",
      "[\"Can't\", 'beat_', 'that', '_', 'sound']\n"
     ]
    }
   ],
   "source": [
    "#The RegexpTokenizer class works by compiling your pattern, then calling re.findall() on your text\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\") # tokenizer is a class object. gaps=True: regex used to identify gaps to tokenize on. gaps=False regex used to identify tokens.\n",
    "\n",
    "tok = tokenizer.tokenize(\"Can't beat_ that _ sound\")\n",
    "\n",
    "print(tok)\n",
    "\n",
    "\n",
    "\n",
    "# same but using a helper function instead of instantiating a class object\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "tok = regexp_tokenize(\"Can't beat_ that _ sound\", \"[\\w']+\") # provide the regex as second parameter to the helper function\n",
    "\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check whether a string is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 0), match=''>\n"
     ]
    }
   ],
   "source": [
    "obj=re.search(r'^$', '')\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dancqueen'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_stem = RegexpStemmer('ing') #stem any word that contains the regex 'ing' whether it occurs as a prefix or suffix\n",
    "\n",
    "regex_stem.stem('dancingqueen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Regular Expressions\n",
    "\n",
    "#### 1. Groups\n",
    "#### 2. Lookahead\n",
    "#### 3. Lookbehind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Groups with ( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'800-555-0000'"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_nb = '800-555-0000'\n",
    "\n",
    "re.search((r'(\\d{3}[ -]?){2}\\d{4}'), phone_nb).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ph Num'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "re.search((r'Ph(one)? Num(ber)?'), 'Phone Number').group()\n",
    "\n",
    "re.search((r'Ph(one)? Num(ber)?'), 'Ph Num').group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive Lookahead ?="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'juice'"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'juice(?=[a-z])', 'juicebar').group() #try juice123 - try removing the lookahead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'juice'"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'juice(?=[0-9]|[a-z]?)', 'juice123').group() #try juice123\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'total'"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'total(?=( tax))', 'total tax').group() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Lookahead ?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'juice'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'juice(?![0-9])', 'juicebar').group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'total'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'total(?!(tax))', 'total tax').group() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive Lookbehind ?<="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bar'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'(?<=[a-z])bar', 'juicebar').group() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Lookbehind ?<!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bar'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'(?<![0-9])bar', 'juicebar').group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get synonyms from WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WordNet** is a lexical database for the English language. In other words, it's a dictionary designed specifically for natural language processing. The wordnet corpus is found in nltk_data/corpora/. \n",
    "\n",
    "**WordNet** includes groups of synonyms with definitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cause emotional anguish or make miserable\n",
      "['It pains me to see my children not being taught well in school']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('pain.n.01'),\n",
       " Synset('pain.n.02'),\n",
       " Synset('pain.n.03'),\n",
       " Synset('pain.n.04'),\n",
       " Synset('annoyance.n.04'),\n",
       " Synset('trouble.v.05'),\n",
       " Synset('pain.v.02')]"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets(\"pain\")\n",
    "\n",
    "# print(syn[1].definition())\n",
    "\n",
    "# print(syn[0].examples())\n",
    "\n",
    "print(syn[6].definition())\n",
    "print(syn[6].examples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having or denoting a low vocal or instrumental range\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('bass.n.01'),\n",
       " Synset('bass.n.02'),\n",
       " Synset('bass.n.03'),\n",
       " Synset('sea_bass.n.01'),\n",
       " Synset('freshwater_bass.n.01'),\n",
       " Synset('bass.n.06'),\n",
       " Synset('bass.n.07'),\n",
       " Synset('bass.n.08'),\n",
       " Synset('bass.s.01')]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wordnet.synsets(\"bass\")\n",
    "print(syn[8].definition())\n",
    "print(syn[0].examples())\n",
    "syn  # synset strings show the <lemma>, <pos>, <sense_number> (most to least frequently used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('natural_language_processing.n.01')]\n",
      "the branch of information science that deals with natural language information\n",
      "[]\n",
      "large Old World boas\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# another example\n",
    "syn = wordnet.synsets(\"NLP\")\n",
    "\n",
    "print(syn)\n",
    "print(syn[0].definition())\n",
    "print(syn[0].examples())\n",
    "\n",
    "syn = wordnet.synsets(\"Python\")\n",
    "print(syn[0].definition())\n",
    "print(syn[0].examples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('music.n.01'), Synset('music.n.02'), Synset('music.n.03'), Synset('music.n.04'), Synset('music.n.05')]\n",
      "an artistic form of auditory communication incorporating instrumental or vocal tones in a structured and continuous manner\n",
      "any agreeable (pleasing and harmonious) sounds\n",
      "[Lemma('music.n.02.music'), Lemma('music.n.02.euphony')]\n"
     ]
    }
   ],
   "source": [
    "syn = wordnet.synsets(\"music\")\n",
    "\n",
    "print(syn)\n",
    "print(syn[0].definition())\n",
    "\n",
    "print(syn[1].definition())\n",
    "print(syn[1].lemmas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Synonymous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Godhead', 'Lord', 'Creator', 'Maker', 'Divine', 'God_Almighty', 'Almighty', 'Jehovah', 'cleric', 'churchman', 'divine', 'ecclesiastic', 'divine', 'divine', 'divine', 'godly', 'providential', 'divine', 'divine', 'godlike', 'divine', 'divine', 'godlike', 'divine', 'elysian', 'inspired']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "\n",
    "for syn in wordnet.synsets('Divine'):\n",
    "\n",
    "    for lemma in syn.lemmas():\n",
    "\n",
    "        synonyms.append(lemma.name())\n",
    "\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Antonyms from WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dark', 'extinguish', 'heavy', 'dark', 'heavy', 'heavy', 'heavy', 'dark', 'heavy', 'heavy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"Light\"):\n",
    "\n",
    "    for l in syn.lemmas():\n",
    "\n",
    "        if l.antonyms():\n",
    "\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech (POS) with Synsets\n",
    "\n",
    "There are four common POS found in WordNet:\n",
    "\n",
    "**Noun n** \n",
    "\n",
    "**Adjective a**\n",
    "\n",
    "**Adverb r**\n",
    "\n",
    "**Verb v**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These **POS tags** can be used for looking up specific synsets for a word. \n",
    "For example, the word bank can be used as a noun or a verb. \n",
    "In WordNet, bank has 10 noun synset and 8 verb synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(wordnet.synsets('dance'))) # total synonyms\n",
    "\n",
    "print(len(wordnet.synsets('dance',pos='n')))\n",
    "print(len(wordnet.synsets('dance',pos='v')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('light.n.01'), Synset('light.n.02'), Synset('light.n.03'), Synset('luminosity.n.01'), Synset('light.n.05'), Synset('light.n.06'), Synset('light.n.07'), Synset('light.n.08'), Synset('light.n.09'), Synset('light.n.10'), Synset('sparkle.n.01'), Synset('light.n.12'), Synset('inner_light.n.01'), Synset('light.n.14'), Synset('lighter.n.02'), Synset('light.v.01'), Synset('light_up.v.05'), Synset('alight.v.01'), Synset('ignite.v.01'), Synset('fall.v.20'), Synset('unhorse.v.01'), Synset('light.a.01'), Synset('light.a.02'), Synset('light.a.03'), Synset('light.a.04'), Synset('light.a.05'), Synset('light.a.06'), Synset('unaccented.s.02'), Synset('light.s.08'), Synset('light.s.09'), Synset('clean.s.03'), Synset('light.s.11'), Synset('light.s.12'), Synset('light.a.13'), Synset('light.a.14'), Synset('faint.s.04'), Synset('light.s.16'), Synset('abstemious.s.02'), Synset('light.s.18'), Synset('light.s.19'), Synset('light.s.20'), Synset('idle.s.04'), Synset('light.s.22'), Synset('light.s.23'), Synset('light.s.24'), Synset('easy.s.10'), Synset('lightly.r.02')]\n",
      "\n",
      " lem = [Lemma('light.n.02.light'), Lemma('light.n.02.light_source')] \n",
      "\n",
      "antonym:  [Lemma('dark.n.01.dark')]\n",
      "antonym:  [Lemma('snuff_out.v.02.extinguish')]\n",
      "antonym:  [Lemma('heavy.a.01.heavy')]\n",
      "antonym:  [Lemma('dark.a.02.dark')]\n",
      "antonym:  [Lemma('heavy.a.03.heavy')]\n",
      "antonym:  [Lemma('heavy.a.02.heavy')]\n",
      "antonym:  [Lemma('heavy.a.04.heavy')]\n",
      "antonym:  [Lemma('dark.a.01.dark')]\n",
      "antonym:  [Lemma('heavy.a.08.heavy')]\n",
      "antonym:  [Lemma('heavy.a.09.heavy')]\n"
     ]
    }
   ],
   "source": [
    "syn = wordnet.synsets('light')\n",
    "print(syn)\n",
    "\n",
    "lem = syn[1].lemmas()  # list of lemmas for synset[1] of light\n",
    "\n",
    "print(\"\\n lem = %s \\n\" % lem)\n",
    "# print(\" lemma = \", lem[1].name())\n",
    "\n",
    "\n",
    "# print(syn[0].lemmas()[0].antonyms())\n",
    "\n",
    "for syn in wordnet.synsets(\"light\"):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            print(\"antonym: \", l.antonyms())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('bank.n.01'), Synset('depository_financial_institution.n.01'), Synset('bank.n.03'), Synset('bank.n.04'), Synset('bank.n.05'), Synset('bank.n.06'), Synset('bank.n.07'), Synset('savings_bank.n.02'), Synset('bank.n.09'), Synset('bank.n.10'), Synset('bank.v.01'), Synset('bank.v.02'), Synset('bank.v.03'), Synset('bank.v.04'), Synset('bank.v.05'), Synset('deposit.v.02'), Synset('bank.v.07'), Synset('trust.v.01')]\n",
      "[Lemma('depository_financial_institution.n.01.depository_financial_institution'), Lemma('depository_financial_institution.n.01.bank'), Lemma('depository_financial_institution.n.01.banking_concern'), Lemma('depository_financial_institution.n.01.banking_company')]\n",
      "banking_company\n"
     ]
    }
   ],
   "source": [
    "syn = wordnet.synsets('bank')\n",
    "\n",
    "len(syn)\n",
    "\n",
    "print(syn)\n",
    "\n",
    "lem = syn[1].lemmas()\n",
    "print(lem)\n",
    "print(lem[3].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "for syn in wordnet.synsets('lazy'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[<bound method Lemma.name of Lemma('lazy.s.01.lazy')>, <bound method Lemma.name of Lemma('faineant.s.01.faineant')>, <bound method Lemma.name of Lemma('faineant.s.01.indolent')>, <bound method Lemma.name of Lemma('faineant.s.01.lazy')>, <bound method Lemma.name of Lemma('faineant.s.01.otiose')>, <bound method Lemma.name of Lemma('faineant.s.01.slothful')>, <bound method Lemma.name of Lemma('faineant.s.01.work-shy')>]\n"
     ]
    }
   ],
   "source": [
    "print(len(synonyms))\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating WordNet Synset similarity\n",
    "#### Return a score denoting how similar two word senses are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cb = wordnet.synset('cookbook.n.01')\n",
    "ib = wordnet.synset('instruction_book.n.01')\n",
    "cb.wup_similarity(ib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bch = wordnet.synset(\"beach.n.01\")\n",
    "oc = wordnet.synset('ocean.n.01')\n",
    "bch.wup_similarity(oc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
