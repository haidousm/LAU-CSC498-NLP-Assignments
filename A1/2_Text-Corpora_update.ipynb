{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.corpus import gutenberg as g #import gutenberg corpus\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize  # word tokenizer\n",
    "from nltk import sent_tokenize # sentence tokenizer\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer # Porter 2\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'child-voice.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 24 26 austen-emma.txt\n",
      "4 26 16 austen-persuasion.txt\n",
      "4 28 22 austen-sense.txt\n",
      "4 33 79 bible-kjv.txt\n",
      "4 19 5 blake-poems.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "4 17 12 burgess-busterbrown.txt\n",
      "4 20 12 carroll-alice.txt\n",
      "4 20 11 chesterton-ball.txt\n",
      "4 22 11 chesterton-brown.txt\n",
      "4 18 10 chesterton-thursday.txt\n",
      "5 27 9 child-voice.txt\n",
      "4 20 24 edgeworth-parents.txt\n",
      "4 25 15 melville-moby_dick.txt\n",
      "4 52 10 milton-paradise.txt\n",
      "4 11 8 shakespeare-caesar.txt\n",
      "4 12 7 shakespeare-hamlet.txt\n",
      "4 12 6 shakespeare-macbeth.txt\n",
      "4 36 12 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "#compute statistics for each text in the corpus\n",
    "# average word length - average sentence length - lexical diversity score (average # times each vocab appears in text)\n",
    "\n",
    "for textid in g.fileids():\n",
    "    nb_chars = len(g.raw(textid))\n",
    "    nb_words = len(g.words(textid))\n",
    "    nb_sents = len(g.sents(textid))\n",
    "    nb_voc = len(set([w.lower() for w in g.words(textid)]))\n",
    "    print ((nb_chars//nb_words), (nb_words//nb_sents), (nb_words//nb_voc), textid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 1.** information retrieved from the above statistics, we get to see that the average word length in English is 4,  however the average sentence length and the lexical diversity is particular to the author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 2.** g.sents(textid): sents() function divides the text up into its sentences, where each sentence is a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112310"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.raw(g.fileids()[-4]) # raw text of 'shakespeare-caesar.txt'\n",
    "\n",
    "len(g.raw(g.fileids()[-4])) # returns the number of letters in the text, including space characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they are not long, the weeping and the laughter, love and desire and hate; i think they have no portion in us after we pass the gate.\n"
     ]
    }
   ],
   "source": [
    "#lower()\n",
    "\n",
    "text = \"They are not LONG, the Weeping and the Laughter, Love and desire and hate; I think they have no portion in us after We pass the gate.\"\n",
    "lower_text = text.lower()\n",
    "print (lower_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are Not LONG, the days of Wine And Roses, Out of a misty DREAM, Our Path Emerges for a While, then closes Within a Dream. - Ernest Downson\n",
      "['They', 'are', 'Not', 'LONG', ',', 'the', 'days', 'of', 'Wine', 'And', 'Roses', ',', 'Out', 'of', 'a', 'misty', 'DREAM', ',', 'Our', 'Path', 'Emerges', 'for', 'a', 'While', ',', 'then', 'closes', 'Within', 'a', 'Dream', '.', '-', 'Ernest', 'Downson']\n"
     ]
    }
   ],
   "source": [
    "text = \"They are Not LONG, the days of Wine And Roses, Out of a misty DREAM, Our Path Emerges for a While, then closes Within a Dream. - Ernest Downson\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(text)\n",
    "print (word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'are', 'Not', 'LONG', ',', 'the', 'days', 'of', 'Wine', 'And', 'Roses', ',', 'Out', 'of', 'a', 'misty', 'DREAM', ',', 'Our', 'Path', 'Emerges', 'for', 'a', 'While', ',', 'then', 'closes', 'Within', 'a', 'Dream', '.', '-', 'Ernest', 'Downson']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"They are Not LONG, the days of Wine And Roses, Out of a misty DREAM, Our Path Emerges for a While, then closes Within a Dream. - Ernest Downson\"\n",
    "\n",
    "#after the above import you use word_tokenize directly\n",
    "word_tokens = word_tokenize(text)\n",
    "print (word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Stop Words**: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine ignores. \n",
    "\n",
    "We would not want these words to take up space in our database, or taking up valuable processing time. \n",
    "\n",
    "For this, NLTK removes them easily by storing a list of words that are considered as *stop words*. NLTKm has a list of stopwords stored in 16 different languages. They are found in the nltk_data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sometimes', 'thousand', 'twangling', 'instruments', 'Will', 'hum', 'mine', 'ears', ';', 'sometime', 'voices', ',', 'That', ',', 'I', 'waked', 'long', 'sleep', ',', 'Will', 'make', 'sleep', ':']\n"
     ]
    }
   ],
   "source": [
    "sample = \"Sometimes a thousand twangling instruments Will hum about mine ears; and sometime voices, That, if I then had waked after long sleep, Will make me sleep again:\"\n",
    "\n",
    "# sample  = \"They are Not LONG, the days of Wine And Roses, Out of a misty DREAM, Our Path Emerges for a While, then closes Within a Dream. - Ernest Downson\"\n",
    "\n",
    "word_tokens = word_tokenize(sample) \n",
    "# word_tokens\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "new_sent = []  # list\n",
    "\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        new_sent.append(w) \n",
    "\n",
    "\n",
    "# print(word_tokens) \n",
    "print(new_sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sometimes', 'thousand', 'twangling', 'instruments', 'Will', 'hum', 'mine', 'ears', ';', 'sometime', 'voices', ',', 'That', ',', 'I', 'waked', 'long', 'sleep', ',', 'Will', 'make', 'sleep', ':']\n"
     ]
    }
   ],
   "source": [
    " # Using List Comprehension\n",
    "    \n",
    "new_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(new_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop words stored in file english.txt inside library 'corpus' of NLTK package\n",
    "\n",
    "\n",
    "# print(stopwords.words('french')) \n",
    "\n",
    "w = stopwords.words('english')\n",
    "\n",
    "res = [True for d in w if (d =='again')]\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# You could add a word to the stop words list\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words.append('hello') #add a stopword\n",
    "\n",
    "print(len(stop_words))\n",
    "# print(stop_words)\n",
    "\n",
    "res = [d for d in stop_words if (d =='hello')]\n",
    "\n",
    "# remove the newly added word from the original stopword list\n",
    "\n",
    "stop_words.remove('hello')\n",
    "\n",
    "print(len(stop_words))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'Sunny', 'Side', 'Up', 'Attitude']\n"
     ]
    }
   ],
   "source": [
    "# add a new list of stop words\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "newstop_words = ['Sunny','Side', 'Up', 'Attitude']\n",
    "stop_words.extend(newstop_words)\n",
    "\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# remove newly added list of stop words\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "newstop_words = ['Sunny','Side', 'Up', 'Attitude']\n",
    "\n",
    "old = [old for old in stop_words if old not in newstop_words]\n",
    "\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find stop words without tokenization\n",
    "\n",
    "def mystopwords(sent):\n",
    "    tokens = sent.split(\" \")\n",
    "    print(tokens)\n",
    "    new_tokens = [w for w in tokens if not w in stop_words]\n",
    "#     print(new_tokens)\n",
    "    return (\" \").join(new_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'there!', 'my', 'old', 'friends!', 'where', 'have', 'you', 'all', 'been.', \"Hadn't\", 'seen', 'you', 'yet.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"hello there! old friends! been. Hadn't seen yet.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystopwords(\"hello there! my old friends! where have you all been. Hadn't seen you yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They are Not LONG, the days of Wine And Roses.', 'Out of a misty DREAM!', 'Our Path Emerges for a While, then closes Within a Dream.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# text = \"Be Not Afeard, the Isle is full of Noises. Sounds and Sweet Airs that Give Delight and Hurt Not.\"\n",
    "\n",
    "text = \"They are Not LONG, the days of Wine And Roses. Out of a misty DREAM! Our Path Emerges for a While, then closes Within a Dream.\"\n",
    "sent_token = sent_tokenize(text)\n",
    "print (sent_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They are Not LONG, the days of Wine And Roses.', 'Out of a misty DREAM!', 'Our Path Emerges for a While, then closes Within a Dream.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization _ sentence and word _ together\n",
    "\n",
    "poem = sent_tokenize(text)\n",
    "print(poem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[['They', 'are', 'Not', 'LONG', ',', 'the', 'days', 'of', 'Wine', 'And', 'Roses', '.'], ['Out', 'of', 'a', 'misty', 'DREAM', '!'], ['Our', 'Path', 'Emerges', 'for', 'a', 'While', ',', 'then', 'closes', 'Within', 'a', 'Dream', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "\n",
    "Tokens = [word_tokenize(t) for t in poem]\n",
    "print(Tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['They', 'are', 'Not', 'LONG', 'the', 'days', 'of', 'Wine', 'And', 'Roses'], ['Out', 'of', 'a', 'misty', 'DREAM'], ['Our', 'Path', 'Emerges', 'for', 'a', 'While', 'then', 'closes', 'Within', 'a', 'Dream']]\n"
     ]
    }
   ],
   "source": [
    "no_punc_tok = []\n",
    "\n",
    "for T in Tokens:\n",
    "    no_punc_tok.append([token for token in T if token not in string.punctuation])\n",
    "    \n",
    "print(no_punc_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**: Chopping off letters from the end of a word until the **stem** is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they', 'are', 'not', 'long', 'the', 'day', 'of', 'wine', 'and', 'rose', 'out', 'of', 'a', 'misti', 'dream', 'our', 'path', 'emerg', 'for', 'a', 'while', 'then', 'close', 'within', 'a', 'dream']\n"
     ]
    }
   ],
   "source": [
    "# Using Porter Stemmer algorithm\n",
    " # Porter1 - Other stemming algorithms are snowball and lancaster\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stemmed_tokens = [porter.stem(token) for seq in no_punc_tok for token in seq]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overstemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "univers\n",
      "univers\n",
      "univers\n",
      "univers\n"
     ]
    }
   ],
   "source": [
    "# university, universal, universities, and universe\n",
    "\n",
    "print(porter.stem(\"university\"))\n",
    "print(porter.stem(\"universal\"))\n",
    "print(porter.stem(\"universities\"))\n",
    "print(porter.stem(\"universe\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter vs Snowball vs Lancaster Stemming Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write\n",
      "write\n",
      "writ\n",
      "gener\n",
      "generous\n",
      "gen\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer() # object of PorterStemmer\n",
    "\n",
    "snow = SnowballStemmer('english') #object of SnowballStemmer\n",
    "\n",
    "lan_stem = LancasterStemmer() #object of LancasterStemmer\n",
    "\n",
    "\n",
    "print(porter.stem('writing'))\n",
    "print(snow.stem(\"writing\"))\n",
    "print(lan_stem.stem(\"writing\"))\n",
    "\n",
    "print(porter.stem('generously'))\n",
    "print(snow.stem('generously'))\n",
    "print(lan_stem.stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer.languages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plurals** Test the stemmer on plurals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon', 'plot']\n",
      "\n",
      "\n",
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon', 'plot']\n",
      "\n",
      "\n",
      "['caress', 'fli', 'die', 'mul', 'deny', 'died', 'agree', 'own', 'humbl', 'siz', 'meet', 'stat', 'siez', 'item', 'sens', 'tradit', 'ref', 'colon', 'plot']\n"
     ]
    }
   ],
   "source": [
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied', 'died', 'agreed', 'owned', 'humbled', 'sized','meeting', \n",
    "           'stating', 'siezing', 'itemization', 'sensational', 'traditional', 'reference', 'colonizer','plotted']\n",
    "\n",
    "# using porter 1\n",
    "\n",
    "print([porter.stem(p) for p in plurals])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print([snow.stem(p) for p in plurals])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print([lan_stem.stem(p) for p in plurals])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma**: the root of a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they', 'are', 'not', 'long', 'the', 'day', 'of', 'wine', 'and', 'rose', 'out', 'of', 'a', 'misti', 'dream', 'our', 'path', 'emerg', 'for', 'a', 'while', 'then', 'close', 'within', 'a', 'dream']\n",
      "\n",
      "\n",
      "['They', 'are', 'Not', 'LONG', 'the', 'day', 'of', 'Wine', 'And', 'Roses', 'Out', 'of', 'a', 'misty', 'DREAM', 'Our', 'Path', 'Emerges', 'for', 'a', 'While', 'then', 'close', 'Within', 'a', 'Dream']\n"
     ]
    }
   ],
   "source": [
    " #based on The Porter Stemming Algorithm\n",
    "# from nltk.corpus import wordnet\n",
    "\n",
    "lm = WordNetLemmatizer() # class - check source code\n",
    "\n",
    "\n",
    "print([porter.stem(token) for seq in no_punc_tok for token in seq])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "lemmatized_tokens = [lm.lemmatize(token) for seq in no_punc_tok for token in seq]\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization with POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\n",
      "Books\n",
      "book\n",
      "happily\n",
      "happily\n",
      "love\n",
      "angrily\n",
      "angrily\n"
     ]
    }
   ],
   "source": [
    "print(lm.lemmatize(\"booking\", pos='v'))\n",
    "\n",
    "print(lm.lemmatize(\"Books\", pos=\"n\"))\n",
    "\n",
    "print(lm.lemmatize(\"books\", pos=\"v\"))\n",
    "\n",
    "print(lm.lemmatize(\"happily\", pos=\"n\"))\n",
    "\n",
    "print(lm.lemmatize(\"happily\", pos=\"v\"))\n",
    "\n",
    "print(lm.lemmatize(\"loving\", pos=\"v\"))\n",
    "\n",
    "print(lm.lemmatize(\"angrily\", pos=\"v\"))\n",
    "\n",
    "print(lm.lemmatize(\"angrily\", pos=\"n\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer vs Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angrili\n",
      "angrily\n",
      "happili\n",
      "happily\n",
      "love\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem(\"angrily\"))\n",
    "print(lm.lemmatize(\"angrily\"))\n",
    "\n",
    "print(porter.stem(\"happily\"))\n",
    "print(lm.lemmatize(\"happily\"))\n",
    "\n",
    "print(porter.stem(\"loving\"))\n",
    "print(lm.lemmatize(\"loving\", pos='v')) # default pos is 'NOUN'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'then', ',', 'in', 'Dreaming', ',', 'the', 'Clouds', 'Methought', ',', 'would', 'Open', ',', 'and', 'Show', 'Riches', 'Ready', 'to', 'Drop', 'upon', 'me', ';', 'that', ',', 'When', 'I', 'Waked', ',', 'I', 'Cried', 'to', 'Dream', 'Again', '.']\n",
      "tokens are:  ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
      "lemma:  ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stopword = stopwords.words('english')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"And then, in Dreaming, the Clouds Methought, would Open, and Show Riches Ready to Drop upon me; that, When I Waked, I Cried to Dream Again.\"\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "lemmatized_word = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "\n",
    "print (lemmatized_word)\n",
    "\n",
    "\n",
    "say = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "wordlist = word_tokenize(say)\n",
    "\n",
    "print(\"tokens are: \", wordlist)\n",
    "#Lemmatize\n",
    "\n",
    "lemma = [lemmatizer.lemmatize(wd) for wd in wordlist]\n",
    "\n",
    "print(\"lemma: \", lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\", 'v'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dancqueen'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_stem = RegexpStemmer('ing') #stem any word that contains the regex 'ing' whether it occurs as a prefix or suffix\n",
    "\n",
    "regex_stem.stem('dancingqueen')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
